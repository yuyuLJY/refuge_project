{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                           chat      country state city  views  \\\n",
      "0           0  https://t.me/helpfulinfoforua  Switzerland   NaN  NaN   7159   \n",
      "1           8    https://t.me/ukraine_reborn  Switzerland   NaN  NaN   2803   \n",
      "2          10    https://t.me/ukraine_reborn  Switzerland   NaN  NaN   4659   \n",
      "3          15    https://t.me/ukraine_reborn  Switzerland   NaN  NaN   2084   \n",
      "4          20    https://t.me/ukraine_reborn  Switzerland   NaN  NaN   6558   \n",
      "\n",
      "   forwards  replies                                        messageText  \\\n",
      "0        42      124  üëã–í—ñ—Ç–∞—î–º–æ –Ω–∞ –∫–∞–Ω–∞–ª—ñ –ö–û–†–ò–°–ù–û (HELPFUL) –®–≤–µ–π—Ü–∞—Ä—Å—å...   \n",
      "1        54        0  üá∫üá¶ –ü—ñ–∫–Ω—ñ–∫. –£–∫—Ä–∞—ó–Ω—Å—å–∫–∏–π –±–æ—Ä—â (–ñ–µ–Ω–µ–≤–∞)\\n\\n–ê—Å–æ—Ü—ñ–∞...   \n",
      "2        61       11  üá∫üá¶ –ó–∞–ø–∏—Å –Ω–∞ –ª—ñ—Ç–Ω—ñ –∑–∞—Ö–æ–¥–∏ Passeport vacances \\n...   \n",
      "3        73        0  üá∫üá¶ –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è –∑ –æ—Å–≤—ñ—Ç–∏ —É –®–≤–µ–π—Ü–∞—Ä—ñ—ó\\n\\nOlga B...   \n",
      "4       123        1  üá∫üá¶ –û—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ —Ä–æ–±–æ—Ç–∏ –≤ –®–≤–µ–π—Ü–∞—Ä—ñ—ó \\n\\n–ó–∞–ø—Ä–æ—à—É—î...   \n",
      "\n",
      "  predicted_class messageDate  \n",
      "0    Volunteering  2022-07-14  \n",
      "1       Education  2022-06-07  \n",
      "2       Education  2022-05-20  \n",
      "3       Education  2022-05-13  \n",
      "4    Volunteering  2022-05-13  \n"
     ]
    }
   ],
   "source": [
    "csv_path = '../../data/telegram/scrape.telegram_forwardsGT30_TokensGT100.csv'\n",
    "df_telegram = pd.read_csv(csv_path)\n",
    "df_telegram_copy = df_telegram.copy()\n",
    "\n",
    "df_head_copy = df_telegram_copy.head()\n",
    "print(df_head_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86183\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\86183\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# stopwords, add ukrainian stopwords from .txt file\n",
    "stopWords = stopwords.words('english') \n",
    "for word in stopwords.words('german'):\n",
    "    stopWords.append(word)\n",
    "for word in stopwords.words('french'):\n",
    "    stopWords.append(word)\n",
    "for word in stopwords.words('italian'):\n",
    "    stopWords.append(word)\n",
    "for word in stopwords.words('russian'):\n",
    "    stopWords.append(word)\n",
    "with open(\"../../data/stopwords/stopwords_ua.txt\") as file: #add ukrainian stopwords loaded from .txt file\n",
    "    ukrstopWords = [line.rstrip() for line in file]\n",
    "for stopwords in ukrstopWords:\n",
    "    stopWords.append(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã–í—ñ—Ç–∞—î–º–æ –Ω–∞ –∫–∞–Ω–∞–ª—ñ –ö–û–†–ò–°–ù–û (HELPFUL) –®–≤–µ–π—Ü–∞—Ä—Å—å–∫–æ–≥–æ –ß–µ—Ä–≤–æ–Ω–æ–≥–æ –•—Ä–µ—Å—Ç–∞.\\n–ú–∏ ‚Äî –≥—Ä—É–ø–∞ –≤–æ–ª–æ–Ω—Ç–µ—Ä—ñ–≤, —è–∫–∞ –¥–æ–ø–æ–º–æ–∂–µ –≤–∞–º –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –≤–∞—à—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è. –ú–∏ –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –≤–ø—Ä–æ–¥–æ–≤–∂ 24 –≥–æ–¥–∏–Ω. –í—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –Ω–∞–¥–∞—é—Ç—å—Å—è –∑ –ø–æ–Ω–µ–¥—ñ–ª–∫–∞ –ø–æ –ø‚Äô—è—Ç–Ω–∏—Ü—é. \\n\\n–ü–µ—Ä—à –Ω—ñ–∂ –ø–æ—Å—Ç–∞–≤–∏—Ç–∏ —Å–≤–æ—î –∑–∞–ø–∏—Ç–∞–Ω–Ω—è, –ø—Ä–æ—Å–∏–º–æ –≤–∞—Å —Å–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –≤–µ–±-—Å–∞–π—Ç –ö–û–†–ò–°–ù–û (HELPFUL): helpful.redcross.ch\\n\\n–ù–∞ —Ü—å–æ–º—É –∫–∞–Ω–∞–ª—ñ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É, —Ä–æ—Å—ñ–π—Å—å–∫—É, –∞–Ω–≥–ª—ñ–π—Å—å–∫—É —Ç–∞ –¥–µ—Ä–∂–∞–≤–Ω—ñ –º–æ–≤–∏ –®–≤–µ–π—Ü–∞—Ä—ñ—ó. –ú–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏–º–µ–º–æ –∑–¥–µ–±—ñ–ª—å—à–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–∏–º–∏ –º–æ–≤–∞–º–∏ –®–≤–µ–π—Ü–∞—Ä—ñ—ó —Ç–∞ –ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏–º–µ–º–æ –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–∏ –Ω–∞—à—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—è —Ñ—É–Ω–∫—Ü—ñ—î—é ¬´–ø–µ—Ä–µ–∫–ª–∞–¥—É¬ª, —è–∫—É –ø—Ä–æ–ø–æ–Ω—É—î Telegram. –£–∫–∞–∑—ñ–≤–∫–∏ —Ç–∞ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –Ω–∞–≤–µ–¥–µ–Ω–æ –Ω–∏–∂—á–µ.\\n\\n‚ö†Ô∏è–û—Å–∫—ñ–ª—å–∫–∏ —Ü–µ –ø—É–±–ª—ñ—á–Ω–∏–π –∫–∞–Ω–∞–ª, –≤–∞—à—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –±–∞—á–∏—Ç–∏–º—É—Ç—å –≤—Å—ñ –±–∞–∂–∞—é—á—ñ.\\n–Ø–∫—â–æ —É –≤–∞—Å —î –∑–∞–ø–∏—Ç–∞–Ω–Ω—è, —â–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î —Ä–æ–∑–∫—Ä–∏—Ç—Ç—è –æ—Å–æ–±–∏—Å—Ç–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó, –ø—Ä–æ—Å–∏–º–æ –≤–∞—Å –¥–æ–¥–∞—Ç–∏ –≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∑–Ω–∞—á–æ–∫ ‚úâÔ∏è.\\n–¢–æ–¥—ñ –º–∏ –Ω–∞–¥—ñ—à–ª–µ–º–æ –≤–∞–º –ø—Ä–∏–≤–∞—Ç–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–æ–º—É —á–∞—Ç—ñ.\\n\\nüìç–í—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –¥–µ—è–∫—ñ –ø–∏—Ç–∞–Ω–Ω—è –±—É–ª–∏ –Ω–∞–¥–∞–Ω—ñ –≤ –ø—Ä–∏–≤–∞—Ç–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è—Ö. –ü—Ä–æ—Å–∏–º–æ –≤–∞—Å –ø–∏—Å–∞—Ç–∏ –ø—ñ–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ —Å–µ–∫—Ü—ñ—è–º–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É –∫–∞–Ω–∞–ª—ñ. \\n\\n____\\n\\nüëã–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ –∫–∞–Ω–∞–ª HELPFUL –®–≤–µ–π—Ü–∞—Ä—Å–∫–æ–≥–æ –ö—Ä–∞—Å–Ω–æ–≥–æ –ö—Ä–µ—Å—Ç–∞. \\n–ú—ã - –∫–æ–º–∞–Ω–¥–∞ –≤–æ–ª–æ–Ω—Ç–µ—Ä–æ–≤ –∏ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –≤–∞–º –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã. –ú—ã —Å—Ç–∞—Ä–∞–µ–º—Å—è –¥–∞–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤. –ú—ã –±—É–¥–µ–º –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞ –ø–æ –ø—è—Ç–Ω–∏—Ü—É.  \\n\\n–ü—Ä–µ–∂–¥–µ —á–µ–º –∑–∞–¥–∞—Ç—å —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å, –ø—Ä–æ—Å–∏–º –≤–∞—Å —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥–ª—è–Ω—É—Ç—å –Ω–∞ —Å–∞–π—Ç HELPFUL: helpful.redcross.ch.\\n\\n–ù–∞ —ç—Ç–æ–º –∫–∞–Ω–∞–ª–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏–ª–∏ —à–≤–µ–π—Ü–∞—Ä—Å–∫–∏–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —è–∑—ã–∫–∏. –ú—ã –±—É–¥–µ–º –æ—Ç–≤–µ—á–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —è–∑—ã–∫–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –®–≤–µ–π—Ü–∞—Ä–∏–∏, –∏ –ø—Ä–æ—Å–∏–º –≤–∞—Å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –Ω–∞—à–∏ –æ—Ç–≤–µ—Ç—ã.  –î–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –æ–ø—Ü–∏–µ–π \"–ø–µ—Ä–µ–≤–µ—Å—Ç–∏\", –∏–º–µ—é—â–µ–π—Å—è –≤ Telegram. –°–º–æ—Ç—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –Ω–∏–∂–µ. \\n\\n‚ö†Ô∏è–ü–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–æ—Ç –∫–∞–Ω–∞–ª —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω—ã–º, –≤–∞—à–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å–º–æ–∂–µ—Ç —É–≤–∏–¥–µ—Ç—å –∫–∞–∂–¥—ã–π.\\n–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å, –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞—é—â–∏–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫–∏—Ö-—Ç–æ –ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ—Å–∏–º –≤–∞—Å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–Ω–∞—á–æ–∫ ‚úâÔ∏è –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.\\n–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã –æ—Ç–ø—Ä–∞–≤–∏–º –≤–∞–º –ª–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∑–∞—â–∏—â–µ–Ω–Ω–æ–º —á–∞—Ç–µ.\\n\\n___\\n\\nüëã Welcome to the channel HELPFUL by the Swiss Red Cross.\\nWe are a team of volunteers and will help you find answers to your questions. We aim to respond within 24 hours. The questions will be answered from Monday to Friday. \\n\\nBefore asking your question, we ask you to please first check the HELPFUL website: helpful.redcross.ch\\n\\nThe languages used in this channel are Ukrainian, Russian, English or the Swiss national languages. We will mainly answer using the Swiss national languages and ask you to translate our answers. To do this please use the option ‚Äútranslate‚Äù provided by Telegram. See instructions in the comments below.\\n\\n‚ö†Ô∏è As this channel is public, your comments will be seen by everyone.\\nIf you have a question that requires sharing some personal data, we ask you to send the icon ‚úâÔ∏è in the comment.\\nWe will then send you a private message in a secured chat.\\n\\nüìçSome answers are provided in private messages. We also please ask you to write under the appropriate sections in the main channel.\n",
      "['–≤—ñ—Ç–∞—î–º–æ –∫–∞–Ω–∞–ª—ñ –∫–æ—Ä–∏—Å–Ω–æ helpful —à–≤–µ–π—Ü–∞—Ä—Å—å–∫–æ–≥–æ —á–µ—Ä–≤–æ–Ω–æ–≥–æ —Ö—Ä–µ—Å—Ç–∞n–º–∏ ‚Äî –≥—Ä—É–ø–∞ –≤–æ–ª–æ–Ω—Ç–µ—Ä—ñ–≤ –¥–æ–ø–æ–º–æ–∂–µ –∑–Ω–∞–π—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è', '–º–∏ –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ 24 –≥–æ–¥–∏–Ω', '–≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –Ω–∞–¥–∞—é—Ç—å—Å—è –ø–æ–Ω–µ–¥—ñ–ª–∫–∞ –ø‚Äô—è—Ç–Ω–∏—Ü—é', 'nn–ø–µ—Ä—à –ø–æ—Å—Ç–∞–≤–∏—Ç–∏ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ—Å–∏–º–æ –ø–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –≤–µ–±—Å–∞–π—Ç –∫–æ—Ä–∏—Å–Ω–æ helpful helpfulredcrosschnn–Ω–∞ –∫–∞–Ω–∞–ª—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É —Ä–æ—Å—ñ–π—Å—å–∫—É –∞–Ω–≥–ª—ñ–π—Å—å–∫—É –¥–µ—Ä–∂–∞–≤–Ω—ñ –º–æ–≤–∏ —à–≤–µ–π—Ü–∞—Ä—ñ—ó', '–º–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏–º–µ–º–æ –∑–¥–µ–±—ñ–ª—å—à–æ–≥–æ –¥–µ—Ä–∂–∞–≤–Ω–∏–º–∏ –º–æ–≤–∞–º–∏ —à–≤–µ–π—Ü–∞—Ä—ñ—ó –ø—Ä–æ–ø–æ–Ω—É–≤–∞—Ç–∏–º–µ–º–æ –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ', '–¥–ª—è —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—è —Ñ—É–Ω–∫—Ü—ñ—î—é ¬´–ø–µ—Ä–µ–∫–ª–∞–¥—É¬ª –ø—Ä–æ–ø–æ–Ω—É—î telegram', '—É–∫–∞–∑—ñ–≤–∫–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –Ω–∞–≤–µ–¥–µ–Ω–æ –Ω–∏–∂—á–µnn–æ—Å–∫—ñ–ª—å–∫–∏ –ø—É–±–ª—ñ—á–Ω–∏–π –∫–∞–Ω–∞–ª –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –±–∞—á–∏—Ç–∏–º—É—Ç—å –±–∞–∂–∞—é—á—ñn—è–∫—â–æ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–∞—î —Ä–æ–∑–∫—Ä–∏—Ç—Ç—è –æ—Å–æ–±–∏—Å—Ç–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ—Å–∏–º–æ –¥–æ–¥–∞—Ç–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∑–Ω–∞—á–æ–∫ n—Ç–æ–¥—ñ –Ω–∞–¥—ñ—à–ª–µ–º–æ –ø—Ä–∏–≤–∞—Ç–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–æ–º—É —á–∞—Ç—ñnn–≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –ø–∏—Ç–∞–Ω–Ω—è –Ω–∞–¥–∞–Ω—ñ –ø—Ä–∏–≤–∞—Ç–Ω–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è—Ö', '–ø—Ä–æ—Å–∏–º–æ –ø–∏—Å–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ —Å–µ–∫—Ü—ñ—è–º–∏ –æ—Å–Ω–æ–≤–Ω–æ–º—É –∫–∞–Ω–∞–ª—ñ', 'nnnn–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –∫–∞–Ω–∞–ª helpful —à–≤–µ–π—Ü–∞—Ä—Å–∫–æ–≥–æ –∫—Ä–∞—Å–Ω–æ–≥–æ –∫—Ä–µ—Å—Ç–∞', 'n–º—ã  –∫–æ–º–∞–Ω–¥–∞ –≤–æ–ª–æ–Ω—Ç–µ—Ä–æ–≤ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç—ã –≤–æ–ø—Ä–æ—Å—ã', '–º—ã —Å—Ç–∞—Ä–∞–µ–º—Å—è –¥–∞–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤', '–º—ã –æ—Ç–≤–µ—á–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞ –ø—è—Ç–Ω–∏—Ü—É', 'nn–ø—Ä–µ–∂–¥–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø—Ä–æ—Å–∏–º –∑–∞–≥–ª—è–Ω—É—Ç—å —Å–∞–π—Ç helpful helpfulredcrosschnn–Ω–∞ –∫–∞–Ω–∞–ª–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —Ä—É—Å—Å–∫–∏–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —à–≤–µ–π—Ü–∞—Ä—Å–∫–∏–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —è–∑—ã–∫–∏', '–º—ã –æ—Ç–≤–µ—á–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —è–∑—ã–∫–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —à–≤–µ–π—Ü–∞—Ä–∏–∏ –ø—Ä–æ—Å–∏–º –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –æ—Ç–≤–µ—Ç—ã', '–¥–ª—è –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –æ–ø—Ü–∏–µ–π –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –∏–º–µ—é—â–µ–π—Å—è telegram', '—Å–º–æ—Ç—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –Ω–∏–∂–µ', 'nn–ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞–Ω–∞–ª —è–≤–ª—è–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —É–≤–∏–¥–µ—Ç—å –∫–∞–∂–¥—ã–πn–µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞—é—â–∏–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫–∏—Ö—Ç–æ –ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Å–∏–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–Ω–∞—á–æ–∫  –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏n–ø–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∏–º –ª–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∑–∞—â–∏—â–µ–Ω–Ω–æ–º —á–∞—Ç–µnnnn welcome channel helpful swiss red crossnwe team volunteers help find answers questions', 'we aim respond 24 hours', 'the questions answered monday friday', 'nnbefore asking question ask check helpful website helpfulredcrosschnnthe languages channel ukrainian russian english swiss national languages', 'we mainly answer swiss national languages ask translate answers', 'to use option ‚Äútranslate‚Äù provided telegram', 'see instructions comments belownn as channel public comments seen everyonenif question requires sharing personal data ask send icon  commentnwe send private message secured chatnnsome answers provided private messages', 'we ask write appropriate sections main channel']\n"
     ]
    }
   ],
   "source": [
    "print(df_telegram_copy['messageText'][0])\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    if len(text) > 50:\n",
    "        new_text = []\n",
    "        for word in text.split():\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        return ' '.join(new_text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# removing HTML Tags\n",
    "def remove_html(text):\n",
    "    if len(text) > 50:\n",
    "        remove_ = re.compile('<.*?')\n",
    "        return re.sub(remove_, r'', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# removing URLs\n",
    "def remove_url(text):\n",
    "    if len(text) > 50:\n",
    "        re_url = re.compile('https?://\\S+|www\\.\\S+')\n",
    "        return re_url.sub('', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# removing emojis, from git\n",
    "def remove_emojis(text):\n",
    "    if len(text) > 50:\n",
    "        emoji_pattern = re.compile(pattern=\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "            u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "            u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "            u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "            u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "            u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# lowercasing\n",
    "def convert_lowercase(text):\n",
    "    if len(text) > 50:\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# removing punctuation\n",
    "def remove_punctuation(sentences):\n",
    "    cleaned_sentences = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.translate(translator)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "    return cleaned_sentences\n",
    "\n",
    "# tokenization \n",
    "def tokenize(text):\n",
    "    if len(text) > 50:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = sent_tokenize(text)\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df_telegram_copy['messageText'] = df_telegram_copy['messageText'].apply(remove_stopwords).apply(remove_html).apply(remove_url).apply(remove_emojis).apply(convert_lowercase).apply(tokenize).apply(remove_punctuation)\n",
    "\n",
    "print(df_telegram_copy['messageText'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18 –ª–∏–ø–Ω—è —î–≤—Ä–æ–ø–µ–π—Å—å–∫–∏–π –ø–∞—Ä–ª–∞–º–µ–Ω—Ç –ø—ñ–¥—Ç—Ä–∏–º–∞–≤ —Ä—ñ—à–µ–Ω–Ω—è —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –≤–∏–∑–Ω–∞–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –ø–æ—Å–≤—ñ–¥—á–µ–Ω—å –≤–æ–¥—ñ—è',\n",
       " 'n n–≤—ñ–¥—Ç–µ–ø–µ—Ä —î—Å –º–æ–∂–Ω–∞ n n –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–º –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è–º –≤–æ–¥—ñ—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –æ–±–º—ñ–Ω—É –ø–µ—Ä—ñ–æ–¥—É –ø–µ—Ä–µ–±—É–≤–∞–Ω–Ω—è –∑–∞—Ö–∏—Å—Ç–æ–º n n –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—è –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è–º–∏ –≤–æ–¥—ñ—è —Å—Ç–∞—Ä–æ–≥–æ –∑—Ä–∞–∑–∫–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–º–∏ –∫–∏—Ä–∏–ª–∏—Ü–µ—é –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∑–∞–≤—ñ—Ä–µ–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É –º—ñ–∂–Ω–∞—Ä–æ–¥–Ω–æ–≥–æ –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è –≤–æ–¥—ñ—è',\n",
       " '–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è –æ—Å–æ–±–∏ n n  –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –≤–∏–¥–∞–Ω–∏–º–∏ –≤–ø–µ—Ä—à–µ 2 —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏–º –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è–º –≤–æ–¥—ñ—è —Ä–∞–∑—ñ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è —Ç–µ—Ä–º—ñ–Ω—É –¥—ñ—ó n n –æ—Ç—Ä–∏–º–∞—Ç–∏ —Å–∫–ª–∞–¥–∞–Ω–Ω—è —ñ—Å–ø–∏—Ç—ñ–≤ —Ç–∏–º—á–∞—Å–æ–≤–µ –Ω–∞ –ø–µ—Ä—ñ–æ–¥ –ø–µ—Ä–µ–±—É–≤–∞–Ω–Ω—è –∑–∞—Ö–∏—Å—Ç–æ–º –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è –≤–æ–¥—ñ—è –∑—Ä–∞–∑–∫–∞ —î—Å —Ä–∞–∑—ñ –≤—Ç—Ä–∞—Ç–∏ –≤–∏–∫—Ä–∞–¥–µ–Ω–Ω—è—É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –ø–æ—Å–≤—ñ–¥—á–µ–Ω–Ω—è –≤–æ–¥—ñ—è',\n",
       " 'n n—á–µ—Ä–µ–∑ 5 –¥–Ω—ñ–≤ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç –Ω–∞–±—É–¥–µ —á–∏–Ω–Ω–æ—Å—Ç—ñn n–ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ —î–≤—Ä–æ–ø–µ–π—Å—å–∫—ñ –∫–æ–ª–µ–≥–∏ –∑–º–æ–∂—É—Ç—å –æ–Ω–ª–∞–π–Ω —Å–∞–π—Ç—ñ —Å–µ—Ä–≤—ñ—Å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä—É –º–≤—Å –∑–∞—Å—Ç–æ—Å—É–Ω–æ–∫ –¥—ñ—è',\n",
       " 'n–¥–µ—Ç–∞–ª—å–Ω—ñ—à–µ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_telegram_copy['messageText'][90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TfIdfTextSummarizer:\n",
    "    def __init__(self):\n",
    "        # Initialize with TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        \n",
    "    # Calculate TF-IDF similarity\n",
    "    def calculate_similarity(self, sentence1, sentence2):\n",
    "        if not sentence1.strip() or not sentence2.strip():\n",
    "            return 0.0\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([sentence1, sentence2])\n",
    "            similarity_score = (tfidf_matrix * tfidf_matrix.T).toarray()[0, 1]\n",
    "            return similarity_score\n",
    "        except Exception as e:\n",
    "            print(\"Error calculating similarity for the following sentences:\")\n",
    "            print(\"Sentence 1:\", sentence1)\n",
    "            print(\"Sentence 2:\", sentence2)\n",
    "            print(\"Error message:\", str(e))\n",
    "            return 0.0\n",
    "        \n",
    "    # Build TextRank graph\n",
    "    def build_text_rank_graph(self, sentences):\n",
    "        G = nx.Graph() \n",
    "        for sentence in sentences:\n",
    "            G.add_node(sentence)\n",
    "        for sentence1 in sentences:\n",
    "            for sentence2 in sentences:\n",
    "                if sentence1 != sentence2:\n",
    "                    similarity_score = self.calculate_similarity(sentence1, sentence2)\n",
    "                    G.add_edge(sentence1, sentence2, weight=similarity_score)\n",
    "        return G\n",
    "\n",
    "    # generate summarization with textrank\n",
    "    def text_rank_summary(self, text):\n",
    "        sentences = text\n",
    "        graph = self.build_text_rank_graph(sentences)\n",
    "        scores = nx.pagerank(graph, weight='weight')\n",
    "        max_score = max(scores.values())\n",
    "        top_sentences = [sentence for sentence, score in scores.items() if score == max_score]\n",
    "        summary = top_sentences[0] \n",
    "        return summary, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../data/summarization/prediction_textrank-sum.csv'\n",
    "df_telegram_copy_2 = df_telegram.copy()\n",
    "summarizer = TfIdfTextSummarizer()\n",
    "df_telegram_copy_2['summary'], df_telegram_copy_2['max_score'] = zip(*df_telegram_copy['messageText'].apply(summarizer.text_rank_summary))\n",
    "df_telegram_copy_2.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank with Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "class BertTextSummarizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        \n",
    "    # calculate sentence embeddings using bert\n",
    "    def embedding_bert(self, sentence):\n",
    "        input_ids = self.tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze(0).numpy()\n",
    "        return sentence_embedding\n",
    "    \n",
    "    # Build TextRank graph using BERT embeddings\n",
    "    def build_text_rank_graph(self, sentences):\n",
    "        G = nx.Graph()\n",
    "        num_sentences = len(sentences)\n",
    "        sentence_embedding = [self.embedding_bert(sentence) for sentence in sentences]\n",
    "        for i in range(num_sentences):\n",
    "            for j in range(i + 1, num_sentences):\n",
    "                embedding1 = sentence_embedding[i]\n",
    "                embedding2 = sentence_embedding[j]\n",
    "                similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "                G.add_edge(sentences[i], sentences[j], weight=similarity_score)\n",
    "        return G\n",
    "    \n",
    "    # generate summarization with textrank\n",
    "    def text_rank_summary(self, text):\n",
    "        sentences = text\n",
    "        graph = self.build_text_rank_graph(sentences)\n",
    "        scores = nx.pagerank(graph, weight='weight')\n",
    "        if not scores:\n",
    "            return \"\", 0.0\n",
    "        max_score = max(scores.values())\n",
    "        top_sentences = [sentence for sentence, score in scores.items() if score == max_score]\n",
    "        summary = top_sentences[0] \n",
    "        return summary, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "output_path = '../../data/summarization/prediction_textrank-sum-bert.csv'\n",
    "df_telegram_copy_bert = df_telegram.copy()\n",
    "summarizer_bert = BertTextSummarizer()\n",
    "df_telegram_copy_bert['summary'], df_telegram_copy_bert['max_score'] = zip(*df_telegram_copy['messageText'].apply(summarizer_bert.text_rank_summary))\n",
    "df_telegram_copy_bert.to_csv(output_path, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
